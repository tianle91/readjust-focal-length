{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from PIL import ExifTags\n",
    "\n",
    "img = Image.open(\"sample.jpg\")\n",
    "depth = Image.open(\"sample-depth.jpg\")\n",
    "\n",
    "# focal length\n",
    "exif = {ExifTags.TAGS[k]: v for k, v in img._getexif().items() if k in ExifTags.TAGS}\n",
    "old_focal_mm = exif[\"FocalLength\"]\n",
    "new_focal_mm_multiplier = 3.\n",
    "if new_focal_mm_multiplier <= 1.:\n",
    "    raise ValueError('new_focal_mm_multiplier cannot be <= 1')\n",
    "new_focal_mm = float(new_focal_mm_multiplier * old_focal_mm)\n",
    "print(f'{old_focal_mm} -> {new_focal_mm}')\n",
    "\n",
    "# resize\n",
    "img = img.resize(size=depth.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "img_arr = np.asarray(img.convert(\"RGBA\"))\n",
    "depth_arr = np.asarray(depth)[:, :, 0] # any channel is the same since it's black and white\n",
    "img_arr.shape, depth_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.hist(depth_arr.reshape(-1), density=True)\n",
    "plt.xlabel('depth')\n",
    "plt.title('Distribution of depths')\n",
    "\n",
    "# depth is in range(0, 256)\n",
    "assert 0 == depth_arr.min() and depth_arr.max() == 255 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(img_arr[:, :, 3], density=True)\n",
    "plt.xlabel('alpha')\n",
    "plt.title('Distribution of transparency')\n",
    "assert 255 == img_arr[:, :, 3].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create depth for effect\n",
    "# d > old focal length and d > new focal length\n",
    "# decrease depth_min_multiplier to exaggerate close-up effect\n",
    "depth_min_multiplier, depth_max_multiplier = 2., 10.\n",
    "depth_min = depth_min_multiplier * max(new_focal_mm, old_focal_mm)\n",
    "depth_max = depth_max_multiplier * max(new_focal_mm, old_focal_mm)\n",
    "depth_min, depth_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_arr_transformed = depth_min + (depth_arr / 256) * (depth_max - depth_min)\n",
    "depth_arr_transformed.min(), depth_arr_transformed.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaling(old_f_in_mm, new_f_in_mm, depth_in_mm) -> float:\n",
    "    if depth_in_mm < min(new_f_in_mm, old_f_in_mm):\n",
    "        raise ValueError('Depth cannot be smaller than new_f_in_mm or old_f_in_mm')\n",
    "    res = (new_f_in_mm / (depth_in_mm - new_f_in_mm))\n",
    "    res = res / (old_f_in_mm / (depth_in_mm - old_f_in_mm))\n",
    "    return res\n",
    "\n",
    "\n",
    "# focal length is proportional to image size\n",
    "assert get_scaling(2, 3, 5) > 1\n",
    "assert get_scaling(3, 2, 5) < 1\n",
    "# for long distances, image size change is focal length change (i.e. zooming)\n",
    "assert np.abs(get_scaling(3, 2, 1e9) - 2 / 3) < 1e-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "size_x, size_y = img.size\n",
    "\n",
    "# in RGBA format\n",
    "new_img_arr = np.zeros(shape=img_arr.shape, dtype=np.uint8)\n",
    "\n",
    "# start writing the furthest transformed pixels\n",
    "for d in tqdm(sorted(np.unique(depth_arr_transformed), reverse=True)):\n",
    "\n",
    "    # this modified scaling factor preserves unity at infinity\n",
    "    scaling = get_scaling(old_f_in_mm=old_focal_mm, new_f_in_mm=new_focal_mm, depth_in_mm=d)\n",
    "    scaling = scaling / (new_focal_mm / old_focal_mm)\n",
    "    new_size_x, new_size_y = int(size_x * scaling), int(size_y * scaling)\n",
    "\n",
    "    mask_arr = depth_arr_transformed == d\n",
    "    img_arr_d = np.zeros(shape=img_arr.shape, dtype=np.uint8)\n",
    "    img_arr_d[mask_arr] = img_arr[mask_arr]\n",
    "\n",
    "    img_d = (\n",
    "        Image\n",
    "        .fromarray(img_arr_d)\n",
    "        .resize((new_size_x, new_size_y))\n",
    "        .crop(box=(\n",
    "            (new_size_x - size_x) // 2,\n",
    "            (new_size_y - size_y) // 2,\n",
    "            size_x,\n",
    "            size_y\n",
    "        ))\n",
    "        .resize((size_x, size_y))\n",
    "    )\n",
    "    img_arr_d = np.asarray(img_d)\n",
    "    new_img_arr[img_arr_d > 0] = img_arr_d[img_arr_d > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transparent_pixels = len(list(zip(*np.where(new_img_arr[:, :, 3] == 0))))\n",
    "\n",
    "f'Need to fill {num_transparent_pixels} ({100*num_transparent_pixels/depth_arr.size:.2f}%) pixels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(new_img_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_img = Image.fromarray(new_img_arr).convert(\"RGB\")\n",
    "new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_img_mask = np.zeros(shape=img_arr.shape, dtype=np.uint8)\n",
    "new_img_mask[new_img_arr[:, :, 3] == 0] = 255\n",
    "new_img_mask = Image.fromarray(new_img_mask).convert(\"RGB\")\n",
    "new_img_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you're logged in with `huggingface-cli login`\n",
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "import torch\n",
    "\n",
    "pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    \"CompVis/stable-diffusion-v1-4\",\n",
    "    revision=\"fp16\",\n",
    "    # torch_dtype=torch.float16,\n",
    "    use_auth_token=True\n",
    ").to('mps')\n",
    "\n",
    "image = pipe(prompt='a photograph', init_image=new_img, mask_image=new_img_mask).images[0]\n",
    "image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51a1e90525d78d04a3cb8c920c0c445e5dac87a06a27b4d347b7720fe82373ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
